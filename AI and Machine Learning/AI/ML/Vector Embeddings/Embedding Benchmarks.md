## Overview

I ran a benchmark to compare the cosine similarity scores of vector embeddings generated by different models. The models tested were a range of SBERT models and a Gemini embedding model (text-embedding-004).

> **_Note:_** For more information on why I think cosine similarity is more suited here than alternatives such as Euclidean distance you can read here: [[Cosine vs Euclidean]]

## Test data

The input data consisted of pairs of questions and responses related to Git commands:

```python 
mockData: List[Dict[str, str]] = [
    {"cachedQuery": "How do I create a new branch in git?",
     "llmResponse": "To create a new branch, use 'git branch <branch-name>' or 'git checkout -b <branch-name>'"},
    {"cachedQuery": "What is the difference between git merge and git rebase?",
     "llmResponse": "Git merge combines changes from different branches, while rebase moves commits to a new base commit"},
    {"cachedQuery": "How can I undo the last commit in Git?",
     "llmResponse": "Use 'git reset HEAD~1' to undo the last commit but keep changes, or 'git reset --hard HEAD~1' to remove changes completely"},
    {"cachedQuery": "What does git stash do?",
     "llmResponse": "Git stash temporarily saves changes that are not ready to be committed, allowing you to switch branches"},
    {"cachedQuery": "How do I resolve merge conflicts in Git?",
     "llmResponse": "Open the conflicting files, manually edit the conflicts, stage the files, and then complete the merge"}
]
```

I used a simple one line user input:

```python
uuser_query = "How do I stash my changes in git?"
```

## Results

````python
 # all-MiniLM-L6-v2:
 Average Similarity: 0.4746
 Individual Similarities:
  Query 1: 0.2351
  Query 2: 0.3960
  Query 3: 0.5785
  Query 4: 0.6393
  Query 5: 0.5243

 # all-mpnet-base-v2:
 Average Similarity: 0.4306
 Individual Similarities:
  Query 1: 0.3858
  Query 2: 0.3213
  Query 3: 0.4272
  Query 4: 0.6269
  Query 5: 0.3919

 # paraphrase-MiniLM-L3-v2:
 Average Similarity: 0.4652
 Individual Similarities:
  Query 1: 0.1377
  Query 2: 0.4488
  Query 3: 0.5181
  Query 4: 0.7813
  Query 5: 0.4403

 # Gemini Embeddings (text-embedding-004):
 Average Similarity: 0.6232
 Individual Similarities:
  Query 1: 0.4840
  Query 2: 0.5591
  Query 3: 0.6396
  Query 4: 0.8589
  Query 5: 0.5745
````

From the results we can see that the Gemini Embeddings performed best out of the group.

> **_Note:_** For more information on why I think Gemini text-embedding-004 was most effective you can read here: [[SBERT vs Gemini Embeddings]]

## Lexical Benchmarks

EXPAND THIS TO INCLUDE THE OTHER LEXICAL BENCHMARKS AND WHAT THE DIFFERENCES ARE